{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d62e192a-32aa-416b-a2ce-676c01f3654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Input, Output, Dataset, Model, Metrics, ClassificationMetrics, Artifact\n",
    "import kfp.kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35704bc5-5fd5-4ef0-99f0-a31d6750ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"matteobrina99999/bonfiglioli_iris:latest\")\n",
    "def lettura(dataset_artifact: Output[Dataset]):\n",
    "    import boto3\n",
    "    import os\n",
    "    import shutil\n",
    "    import pandas as pd\n",
    "\n",
    "    s3 = boto3.client('s3',\n",
    "                      endpoint_url=os.getenv('S3_ENDPOINT'),\n",
    "                      aws_access_key_id=os.getenv('S3_ACCESS_KEY_ID'),\n",
    "                      aws_secret_access_key=os.getenv('S3_SECRET_ACCESS_KEY'))\n",
    "    \n",
    "    s3.download_file('iris-bucket', 'iris_example/IRIS.csv', 'IRIS.csv')\n",
    "    iris_data=pd.read_csv('IRIS.csv')\n",
    "    print(iris_data.head())\n",
    "    shutil.move(\"IRIS.csv\", dataset_artifact.path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b80880e-02c0-4328-b2bb-1f7d1cb04dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"matteobrina99999/bonfiglioli_iris:latest\")\n",
    "def preprocessing(dataset_artifact: Input[Dataset],X_train_artifact: Output[Dataset], X_test_artifact: Output[Dataset], y_train_artifact: Output[Dataset], y_test_artifact: Output[Dataset]):\n",
    "    import pandas as pd \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import shutil\n",
    "\n",
    "    iris_data=pd.read_csv(dataset_artifact.path, header=0)\n",
    "    X = iris_data.loc[:, iris_data.columns != 'species']\n",
    "    y = iris_data.loc[:, ['species']]\n",
    "    y_enc = LabelEncoder().fit_transform(y)  #trasforma le etichette testuali in valori numerici\n",
    "    y_label = tf.keras.utils.to_categorical(y_enc)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_label, test_size=0.3)\n",
    "    np.save('/tmp/X_train.npy', X_train)\n",
    "    np.save('/tmp/X_test.npy', X_test)\n",
    "    np.save('/tmp/y_train.npy', y_train)\n",
    "    np.save('/tmp/y_test.npy', y_test)\n",
    "    shutil.move('/tmp/X_train.npy', X_train_artifact.path)\n",
    "    shutil.move('/tmp/X_test.npy', X_test_artifact.path)\n",
    "    shutil.move('/tmp/y_train.npy', y_train_artifact.path)\n",
    "    shutil.move('/tmp/y_test.npy', y_test_artifact.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4701567-8da6-446d-828a-57386aba29c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"matteobrina99999/bonfiglioli_iris:latest\")\n",
    "def model_building(X_train_artifact: Input[Dataset], model_artifact: Output[Model]):\n",
    "    import numpy as np\n",
    "    import shutil\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow import keras\n",
    "    X_train=np.load(X_train_artifact.path)\n",
    "    \n",
    "    model = Sequential([\n",
    "        keras.layers.Input(shape=X_train.shape[1:]),\n",
    "        keras.layers.Dense(1000, activation='relu'),\n",
    "        keras.layers.Dense(500, activation='relu',),\n",
    "        keras.layers.Dense(300, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    \n",
    "\n",
    "    path_parts = model_artifact.path.split(\"/\")\n",
    "\n",
    "    \n",
    "    path_parts[-1] = \"model.h5\"\n",
    "    \n",
    "   \n",
    "    model_artifact.path = \"/\".join(path_parts)\n",
    "    model.save(model_artifact.path, model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ff8394a-b631-42dd-8e65-354182593e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"matteobrina99999/bonfiglioli_iris:latest\")\n",
    "def model_training(X_train_artifact: Input[Dataset], X_test_artifact: Input[Dataset], y_train_artifact: Input[Dataset], y_test_artifact: Input[Dataset],  model_artifact: Input[Model], trained_model_artifact: Output[Model], hyperparameters : dict):\n",
    "    import numpy as np\n",
    "    from tensorflow import keras\n",
    "\n",
    "    epochs=int(hyperparameters['epochs'])\n",
    "    X_train=np.load(X_train_artifact.path)\n",
    "    X_test=np.load(X_test_artifact.path)\n",
    "    y_train=np.load(y_train_artifact.path)\n",
    "    y_test=np.load(y_test_artifact.path)\n",
    "\n",
    "    model=keras.models.load_model(model_artifact.path)\n",
    "    model.compile(optimizer='adam', \n",
    "              loss=keras.losses.CategoricalCrossentropy(),\n",
    "             metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs, validation_data=(X_test, y_test))\n",
    "    path_parts = trained_model_artifact.path.split(\"/\")\n",
    "\n",
    "  \n",
    "    path_parts[-1] = \"trained_model.h5\"\n",
    "    \n",
    "  \n",
    "    trained_model_artifact.path = \"/\".join(path_parts)\n",
    "    model.save(trained_model_artifact.path, model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edaca4d8-8cca-400b-8646-8cd7e61b3347",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"matteobrina99999/bonfiglioli_iris:latest\")\n",
    "def model_conversion(trained_model_artifact: Input[Model], onnx_model_artifact: Output[Model]):\n",
    "    from tensorflow import keras\n",
    "    import tensorflow as tf\n",
    "    import tf2onnx\n",
    "    import onnx\n",
    "    import boto3\n",
    "    import os\n",
    "\n",
    "    model=keras.models.load_model(trained_model_artifact.path)\n",
    "    model.output_names=['output']\n",
    "    input_signature = [tf.TensorSpec([None, 4], tf.float32, name='x')]\n",
    "    onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature)\n",
    "    onnx.save(onnx_model, \"/tmp/model.onnx\")\n",
    "    path_parts = onnx_model_artifact.path.split(\"/\")\n",
    "\n",
    "   \n",
    "    path_parts[-1] = \"model.onnx\"\n",
    "    \n",
    "\n",
    "    onnx_model_artifact.path = \"/\".join(path_parts)\n",
    "    onnx.save(onnx_model, onnx_model_artifact.path)\n",
    "    text= '''name: \"iris\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size: 10000'''\n",
    "    with open(\"/tmp/config.pbtxt\", \"wt\") as file:\n",
    "        file.write(text)\n",
    "    s3 = boto3.client('s3',\n",
    "                      endpoint_url=os.getenv('S3_ENDPOINT'),\n",
    "                      aws_access_key_id=os.getenv('S3_ACCESS_KEY_ID'),\n",
    "                      aws_secret_access_key=os.getenv('S3_SECRET_ACCESS_KEY'))\n",
    "\n",
    "    response = s3.upload_file('/tmp/model.onnx', 'iris-bucket', '/iris_example/iris/1/model.onnx' )\n",
    "    response = s3.upload_file('/tmp/config.pbtxt', 'iris-bucket', '/iris_example/iris/config.pbtxt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77cf037b-73bd-46bc-b959-8e599103fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"matteobrina99999/bonfiglioli_iris:kserve\")\n",
    "def model_serving(onnx_model_artifact : Input[Model]):\n",
    "    \"\"\"\n",
    "    Create kserve instance\n",
    "    \"\"\"\n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1ONNXRuntimeSpec\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    import os\n",
    "    import boto3\n",
    "    \n",
    " \n",
    "    uri = 's3://iris-bucket/iris_example'\n",
    "\n",
    "  \n",
    "    namespace = \"modelli\"\n",
    "    now = datetime.now()\n",
    "    name=\"iris\"\n",
    "    kserve_version='v1beta1'\n",
    "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "    isvc = V1beta1InferenceService(api_version=api_version,\n",
    "                                   kind=constants.KSERVE_KIND,\n",
    "                                   metadata=client.V1ObjectMeta(\n",
    "                                       name=name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false', 'serving.kserve.io/enable-prometheus-scraping': 'true'}),\n",
    "                                   spec=V1beta1InferenceServiceSpec(\n",
    "                                   predictor=V1beta1PredictorSpec(\n",
    "                                       service_account_name=\"sa-minio-kserve\",\n",
    "                                       onnx=(V1beta1ONNXRuntimeSpec(\n",
    "                                           storage_uri=uri))))\n",
    "    )\n",
    "\n",
    "    s3 = boto3.client('s3',\n",
    "                      endpoint_url=os.getenv('S3_ENDPOINT'),\n",
    "                      aws_access_key_id=os.getenv('S3_ACCESS_KEY_ID'),\n",
    "                      aws_secret_access_key=os.getenv('S3_SECRET_ACCESS_KEY'))\n",
    "\n",
    "    s3.download_file('iris-bucket', 'iris_example/config', '/tmp/config')\n",
    "\n",
    "    KServe = KServeClient(config_file='/tmp/config')\n",
    "    \n",
    "  \n",
    "    try:\n",
    "        KServe.delete(name=name, namespace=namespace)\n",
    "        print(\"Modello precedente eliminato\")\n",
    "    except:\n",
    "        print(\"Non posso eliminare\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "    KServe.create(isvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce7c9be9-04fc-4d5f-adbf-26a13050f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='iris',\n",
    "    description='iris-test')\n",
    "def iris_pipeline(hyperparameters: dict):\n",
    "    lettura_task= lettura()\n",
    "    preprocessing_task=preprocessing(\n",
    "        dataset_artifact = lettura_task.outputs[\"dataset_artifact\"]\n",
    "    )\n",
    "    model_building_task=model_building(\n",
    "        X_train_artifact = preprocessing_task.outputs[\"X_train_artifact\"]\n",
    "    )\n",
    "    model_training_task=model_training(\n",
    "        X_train_artifact = preprocessing_task.outputs[\"X_train_artifact\"],\n",
    "        X_test_artifact = preprocessing_task.outputs[\"X_test_artifact\"],\n",
    "        y_train_artifact = preprocessing_task.outputs[\"y_train_artifact\"],\n",
    "        y_test_artifact = preprocessing_task.outputs[\"y_test_artifact\"],\n",
    "        model_artifact = model_building_task.outputs[\"model_artifact\"],\n",
    "        hyperparameters=hyperparameters\n",
    "    )\n",
    "    model_conversion_task=model_conversion(\n",
    "        trained_model_artifact = model_training_task.outputs[\"trained_model_artifact\"]\n",
    "    )\n",
    "\n",
    "    model_serving_task=model_serving(\n",
    "        onnx_model_artifact = model_conversion_task.outputs[\"onnx_model_artifact\"]\n",
    "    )\n",
    "    lettura_task.set_caching_options(False)\n",
    "    preprocessing_task.set_caching_options(False)\n",
    "    model_building_task.set_caching_options(False)\n",
    "    model_training_task.set_caching_options(False)\n",
    "    model_conversion_task.set_caching_options(False)\n",
    "    model_serving_task.set_caching_options(False)\n",
    "    kfp.kubernetes.use_secret_as_env(lettura_task, 'iris-secret', {'S3_ACCESS_KEY_ID':'S3_ACCESS_KEY_ID', 'S3_SECRET_ACCESS_KEY':'S3_SECRET_ACCESS_KEY', 'S3_ENDPOINT':'S3_ENDPOINT'})\n",
    "    kfp.kubernetes.use_secret_as_env(model_conversion_task, 'iris-secret', {'S3_ACCESS_KEY_ID':'S3_ACCESS_KEY_ID', 'S3_SECRET_ACCESS_KEY':'S3_SECRET_ACCESS_KEY', 'S3_ENDPOINT':'S3_ENDPOINT'})\n",
    "    kfp.kubernetes.use_secret_as_env(model_serving_task, 'iris-secret', {'S3_ACCESS_KEY_ID':'S3_ACCESS_KEY_ID', 'S3_SECRET_ACCESS_KEY':'S3_SECRET_ACCESS_KEY', 'S3_ENDPOINT':'S3_ENDPOINT'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dbfa375-7338-4a9a-bd09-b6a64295e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "compiler.Compiler().compile(iris_pipeline, 'pipeline_iris.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kfp_venv",
   "language": "python",
   "name": "kfp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
